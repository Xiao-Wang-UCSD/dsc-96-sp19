{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing\n",
    "\n",
    "Colin Jemmott for UC San Diego, September 2018\n",
    "\n",
    "The file `UCSD tweets.csv` has a small number of tweets from August and September 2018 that contained the term \"UCSD\".  Let's analyze them!\n",
    "\n",
    "### Steps\n",
    "\n",
    "Each step is explained in more detail below\n",
    "1. Open the CSV and explore the data\n",
    "2. Clean the data\n",
    "3. Count word frequency\n",
    "4. Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Open the CSV and explore the data\n",
    "\n",
    "### Steps:\n",
    "* Use Pandas to load the CSV\n",
    "* Examine the data.  How many tweets are there?  How long is the shortest tweet?  Longest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetDf = pd.read_csv(\"UCSD tweets.csv\")\n",
    "tweetDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Clean the data\n",
    "\n",
    "### Filter out tweets that don't have `UCSD` in the text\n",
    "\n",
    "The Twitter search matches on both username and tweet text.  We want just the ones that have a match in the tweet itself.  The result should be a new dataframe with the subset of matching tweets.\n",
    "\n",
    "I recommend using the `.str.contains()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for duplicates\n",
    "\n",
    "See if any of the tweets have exactly the same text.  If so, are they true duplicates?  Does it make sense to remove them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Count Words\n",
    "\n",
    "We want to find out what the most frequent words are, so we need to split things up.  In text this is called tokenizing.\n",
    "\n",
    "### Steps\n",
    "\n",
    "1. Make a single long string with all of the tweet text.  Make sure to put spaces between them!\n",
    "2. Split the tweets into a list of words using `.split()`\n",
    "3. Print out the first 20 words to make sure it looks like what you think it should.\n",
    "\n",
    "How many words are there all together?  How many distinct words? (remember `set()`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove short words\n",
    "\n",
    "Short words are really common, and aren't super helpful for comparing word count.  Usually it is best to remove what are called \"stop words\", which include things like \"of\", \"a\", \"in\", etc.  In this case we will just remove all words that are less than three charecters long.\n",
    "\n",
    "The result should be a new list of words.  How many total?  How many unique?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Count word frequency\n",
    "\n",
    "Run the code below to count word freqnecy.  Are you surprised by the most common?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "counts = Counter(longWordList)\n",
    "print(counts.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.b. Tokenize again (with NLTK this time)\n",
    "\n",
    "Why is UCSD only in 18?  \n",
    "\n",
    "Because of `@UCSD` and similar.  \n",
    "\n",
    "Tokenizing (like most things) is harder than it looks at first!  \n",
    "\n",
    "Generally, a good solution is to use a tool built for the job rather than rolling your own.  In this case, we will use the Python package Natural Language ToolKit, NLTK.  \n",
    "\n",
    "You may need to install NLTK and also download an English language corpus.  If so, do this in the terminal:\n",
    "\n",
    "```\n",
    "pip install --user nltk\n",
    "```\n",
    "\n",
    "Then in Jupyter run this once:\n",
    "```\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "```\n",
    "\n",
    "Run the code below to use NLTK's tokenizer, and then repeat the process of removing short words and counting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordList = tokenize.word_tokenize(allText)\n",
    "len(wordList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.b. Count (again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove short words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment with NLTK\n",
    "\n",
    "What is sentiment?  Why do we care?\n",
    "\n",
    "Will need to run once:\n",
    "```\n",
    "nltk.download('vader_lexicon')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n",
    "sid.polarity_scores(\"Good test!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetSentiments = []\n",
    "\n",
    "for tweet in tweetDf['text']:\n",
    "    tweetSentiment = sid.polarity_scores(tweet)\n",
    "    tweetSentiment['text'] = tweet\n",
    "    tweetSentiments.append(tweetSentiment)\n",
    "tweetSentimentDf = pd.DataFrame.from_records(tweetSentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetSentimentDf.sort_values('compound')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "\n",
    "* Lemmatization / stemming\n",
    "* Word Cloud"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
